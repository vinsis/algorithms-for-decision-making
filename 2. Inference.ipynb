{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference in Bayesian networks\n",
    "\n",
    "We can implement exact inference using factors. Recall that factors represent discrete multivariate distributions. We use the following three operations on factors to achieve this:\n",
    "\n",
    "#### 1. Factor product\n",
    "We use the factor product to combine two factors to produce a larger factor whose scope is the combined scope of the input factors. If we have $φ(X, Y)$ and $ψ(Y, Z)$, then $φ · ψ$ will be over $X$, $Y$, and $Z$ with $(φ · ψ)(x, y, z) = φ(x, y)ψ(y, z)$. \n",
    "\n",
    "#### 2. Factor marginalization\n",
    "We use factor marginalization to sum out a particular variable from the entire factor table, removing it from the resulting scope.\n",
    "\n",
    "#### 3. Factor conditioning\n",
    "We use factor conditioning with respect to some evidence to remove any rows in the table inconsistent with that evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement the code from last notebook (`1. Representations`) so we can reuse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "select (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Variable\n",
    "    name::Symbol\n",
    "    m::Int\n",
    "end\n",
    "\n",
    "const Assignment = Dict{Symbol, Int}\n",
    "const FactorTable = Dict{Assignment, Float64}\n",
    "\n",
    "struct Factor\n",
    "    vars::Vector{Variable}\n",
    "    table::FactorTable\n",
    "end\n",
    "\n",
    "Base.Dict{Symbol, T}(a::NamedTuple) where T = \n",
    "    Dict{Symbol, T}( k=>v for (k,v) in zip(keys(a), values(a)) )\n",
    "\n",
    "Base.convert(::Type{Dict{Symbol, T}}, a::NamedTuple) where T = \n",
    "    Dict{Symbol, T}(a)\n",
    "\n",
    "Base.isequal(a::Dict{Symbol, T}, b::NamedTuple) where T = \n",
    "    length(a) == length(b) &&\n",
    "    all(a[k] == v for (k,v) in zip(keys(b), values(b)))\n",
    "\n",
    "variablenames(ϕ::Factor) = [var.name for var in ϕ.vars]\n",
    "\n",
    "select(a::Assignment, varnames::Vector{Symbol}) = \n",
    "    Assignment( n => a[n] for n in varnames )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assignments (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base.Iterators: product\n",
    "\n",
    "function assignments(vars::AbstractVector{Variable})\n",
    "    names = [var.name for var in vars]\n",
    "    matrix_of_assignments = [Assignment(name => p for (name, p) in zip(names, p)) \n",
    "        for p in product((1:var.m for var in vars)...)]\n",
    "    vec(matrix_of_assignments)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LightGraphs\n",
    "\n",
    "function normalize!(ϕ::Factor)\n",
    "    if isempty(ϕ.table)\n",
    "        return ϕ\n",
    "    end\n",
    "    z = sum(p for (a,p) in ϕ.table)\n",
    "    for (a,p) in ϕ.table\n",
    "        ϕ.table[a] = p/z\n",
    "    end\n",
    "    ϕ\n",
    "end\n",
    "\n",
    "struct BayesianNetwork\n",
    "    vars::Vector{Variable}\n",
    "    factors::Vector{Factor}\n",
    "    graph::SimpleDiGraph{Int64}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Factor product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:*(ϕ::Factor, ψ::Factor)\n",
    "    ϕ_names = variablenames(ϕ)\n",
    "    ψ_names = variablenames(ψ)\n",
    "    ψ_only = setdiff(ψ.vars, ϕ.vars)\n",
    "    final_table = FactorTable()\n",
    "    for (ϕ_assignment, ϕ_prob) in ϕ.table\n",
    "        for ψ_only_assignment in assignments(ψ_only)\n",
    "            complete_assignment = merge(ϕ_assignment, ψ_only_assignment)\n",
    "            ψ_assignment = select(complete_assignment, ψ_names)\n",
    "            final_table[complete_assignment] = ϕ_prob * get(ψ.table, ψ_assignment, 0.0)\n",
    "        end\n",
    "    end\n",
    "    vars = vcat(ϕ.vars, ψ_only)\n",
    "    Factor(vars, final_table)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Factor marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marginalize (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function marginalize(ϕ::Factor, name::Symbol)\n",
    "    final_table = FactorTable()\n",
    "    for (assignment, prob) in ϕ.table\n",
    "        assignment_without_name = delete!(copy(assignment), name)\n",
    "        final_table[assignment_without_name] = get(final_table, assignment_without_name, 0.0) + prob\n",
    "    end\n",
    "    final_variables = filter(v -> v.name != name, ϕ.vars)\n",
    "    Factor(final_variables, final_table)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Factor conditioning\n",
    "\n",
    "Two methods for factor conditioning given some evidence. \n",
    "\n",
    "i) The first takes a factor $φ$ and returns a new factor whose table entries are consistent with the variable named `name` having value `value`. \n",
    "\n",
    "ii) The second takes a factor $φ$ and applies evidence in the form of a named tuple. The `in_scope` method returns `true` if a variable named `name` is within the scope of the factor $φ$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "condition (generic function with 2 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_scope(name::Symbol, ϕ::Factor) = any(name == v.name for v in ϕ.vars)\n",
    "\n",
    "function condition(ϕ::Factor, name::Symbol, value)\n",
    "    if !in_scope(name, ϕ)\n",
    "        return ϕ\n",
    "    end\n",
    "    final_table = FactorTable()\n",
    "    for (assignment, prob) in ϕ.table\n",
    "        if assignment[name] == value\n",
    "            final_table[delete!(copy(assignment), name)] = prob\n",
    "        end\n",
    "    end\n",
    "    final_variables = filter(v -> v.name != name, ϕ.vars)\n",
    "    Factor(final_variables, final_table)\n",
    "end\n",
    "\n",
    "function condition(ϕ::Factor, evidence)\n",
    "    for (name, value) in pairs(evidence)\n",
    "        ϕ = condition(ϕ, name, value)\n",
    "    end\n",
    "    ϕ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact inference\n",
    "\n",
    "A naive exact inference algorithm for a discrete Bayesian network `bn`, which takes as input a set of query variable names `query`, and `evidence` associating values with observed variables. \n",
    "\n",
    "The algorithm computes a joint distribution over the query variables in the form of a factor. \n",
    "\n",
    "> We introduce the `ExactInference` type to allow for `infer` to be called with different inference methods, as shall be seen in the rest of this chapter.\n",
    "\n",
    "Notice how `ExactInference` parameter is not used in the function definition. It is solely used for multiple dispatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ExactInference end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "infer (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function infer(::ExactInference, bn, query, evidence)\n",
    "    ϕ = prod(bn.factors)\n",
    "    ϕ = condition(ϕ, evidence)\n",
    "    for name in setdiff(variablenames(ϕ), query)\n",
    "        ϕ = marginalize(ϕ, name)\n",
    "    end\n",
    "    normalize!(ϕ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test on the example given in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(:x, 2)\n",
    "Y = Variable(:y, 2)\n",
    "Z = Variable(:z, 2)\n",
    "\n",
    "ϕ1 = Factor([X,Y], FactorTable(\n",
    "        (x=1, y=1) => 0.3,\n",
    "        (x=1, y=2) => 0.4,\n",
    "        (x=2, y=1) => 0.2,\n",
    "        (x=2, y=2) => 0.1,\n",
    "        ))\n",
    "\n",
    "ϕ2 = Factor([Y,Z], FactorTable(\n",
    "        (y=1, z=1) => 0.2,\n",
    "        (y=1, z=2) => 0.0,\n",
    "        (y=2, z=1) => 0.3,\n",
    "        (y=2, z=2) => 0.5,\n",
    "        ));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test factor product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable[Variable(:x, 2), Variable(:y, 2), Variable(:z, 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Dict{Symbol, Int64}, Float64} with 8 entries:\n",
       "  Dict(:y=>1, :z=>1, :x=>1) => 0.06\n",
       "  Dict(:y=>1, :z=>1, :x=>2) => 0.04\n",
       "  Dict(:y=>1, :z=>2, :x=>1) => 0.0\n",
       "  Dict(:y=>2, :z=>2, :x=>1) => 0.2\n",
       "  Dict(:y=>2, :z=>1, :x=>2) => 0.03\n",
       "  Dict(:y=>1, :z=>2, :x=>2) => 0.0\n",
       "  Dict(:y=>2, :z=>2, :x=>2) => 0.05\n",
       "  Dict(:y=>2, :z=>1, :x=>1) => 0.12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ϕ1 * ϕ2\n",
    "println(result.vars)\n",
    "result.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test factor marginalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(:x, 2)\n",
    "Y = Variable(:y, 2)\n",
    "Z = Variable(:z, 2)\n",
    "\n",
    "ϕ = Factor([X,Y,Z], FactorTable(\n",
    "        (x=1, y=1, z=1) => 0.08,\n",
    "        (x=1, y=1, z=2) => 0.31,\n",
    "        (x=1, y=2, z=1) => 0.09,\n",
    "        (x=1, y=2, z=2) => 0.37,\n",
    "        (x=2, y=1, z=1) => 0.01,\n",
    "        (x=2, y=1, z=2) => 0.05,\n",
    "        (x=2, y=2, z=1) => 0.02,\n",
    "        (x=2, y=2, z=2) => 0.07,\n",
    "        ));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable[Variable(:x, 2), Variable(:z, 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Dict{Symbol, Int64}, Float64} with 4 entries:\n",
       "  Dict(:z=>2, :x=>2) => 0.12\n",
       "  Dict(:z=>2, :x=>1) => 0.68\n",
       "  Dict(:z=>1, :x=>1) => 0.17\n",
       "  Dict(:z=>1, :x=>2) => 0.03"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = marginalize(ϕ, Y.name)\n",
    "println(result.vars)\n",
    "result.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test factor conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable[Variable(:x, 2), Variable(:z, 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Dict{Symbol, Int64}, Float64} with 4 entries:\n",
       "  Dict(:z=>2, :x=>2) => 0.07\n",
       "  Dict(:z=>2, :x=>1) => 0.37\n",
       "  Dict(:z=>1, :x=>1) => 0.09\n",
       "  Dict(:z=>1, :x=>2) => 0.02"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = condition(ϕ, :y, 2.0)\n",
    "println(result.vars)\n",
    "result.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable[Variable(:z, 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Dict{Symbol, Int64}, Float64} with 2 entries:\n",
       "  Dict(:z=>1) => 0.09\n",
       "  Dict(:z=>2) => 0.37"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = condition(ϕ, (y=2,x=1))\n",
    "println(result.vars)\n",
    "result.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:y, 2), Variable(:z, 2)], Dict(Dict(:y => 2, :z => 2) => 0.37, Dict(:y => 1, :z => 1) => 0.08, Dict(:y => 1, :z => 2) => 0.31, Dict(:y => 2, :z => 1) => 0.09))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition(ϕ, (g=2,x=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference in Naive Bayes models\n",
    "\n",
    "![](./assets/ex_3.3.png)\n",
    "\n",
    "> We have to specify the prior $P(C)$ and the class-conditional distributions $P(O_i|C)$\n",
    "\n",
    "> Our classification task involves computing the conditional probability $P(c | o_{1:n})$\n",
    "\n",
    "$$P(c | o_{1:n}) = \\frac{P(c, o_{1:n})}{P(o_{1:n})}$$\n",
    "\n",
    "We can compute the denominator by marginalizing the joint distribution. The denominator is not a function of $C$ and can therefore be treated as a constant. Thus we can write\n",
    "\n",
    "$$P(c | o_{1:n}) \\propto P(c, o_{1:n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum-product variable elimination\n",
    "\n",
    "Consider the below figure:\n",
    "\n",
    "![](./assets/ex_2.5.png)\n",
    "\n",
    "Let's say we want to compute the distribution $P(B | d^1, c^1)$\n",
    "\n",
    "The conditional probability distributions associated with the nodes in the network can be __represented__ by the following factors:\n",
    "\n",
    "$$ϕ_1(B), ϕ_2(S), ϕ_3(E,B,S), ϕ_4(D,E), ϕ_5(C,E)$$\n",
    "\n",
    "Because $D$ and $C$ are observed variables, the last two factors can be replaced with $φ_6(E)$ and $φ_7(E)$ by setting the evidence $D = 1$ and $C = 1$.\n",
    "\n",
    "We eliminate $E$ then $S$:\n",
    "\n",
    "$$φ_8(B,S) = ∑_e φ_3(e,B,S)φ_6(e)φ_7(e)$$\n",
    "$$φ_9(B) = ∑φ_2(s)φ_8(B,s)$$\n",
    "\n",
    "Finally we take the product of $φ_9(B)$ and $φ_1(B)$ and normalize it to get $P(B|d^1,c^1)$\n",
    "\n",
    "__Previous way__: Taking the product of all factors then marginalizing\n",
    "\n",
    "$$ P(B | d^1,c^1) ∝ ∑_s ∑_e φ_1(B)φ_2(s)φ_3(e | B,s)φ_4(d^1 | e)φ_5(c^1 | e) $$\n",
    "\n",
    "\n",
    "__Sum product way__:\n",
    "\n",
    "$$ P(B | d^1, c^1) ∝ φ_1(B) ∑_s( φ_2(s) ∑_e (φ_3(e | B, s) φ_4(d^1 | e) φ_5(c^1 | e)) )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "infer (generic function with 2 methods)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct VariableElimination\n",
    "    ordering # array of variable indices\n",
    "end\n",
    "\n",
    "function infer(M::VariableElimination, bn, query, evidence)\n",
    "    ϕs_after_evidence = [condition(ϕ, evidence) for ϕ in bn.factors]\n",
    "    for index in M.ordering\n",
    "        name = bn.vars[index].name\n",
    "        if name ∉ query\n",
    "            indices_containing_name = findall(ϕ -> in_scope(name, ϕ), ϕs_after_evidence)\n",
    "            if !isempty(indices_containing_name)\n",
    "                ϕ = prod(ϕs_after_evidence[indices_containing_name])\n",
    "                deleteat!(ϕs_after_evidence, indices_containing_name)\n",
    "                ϕ_after_marginalizing = marginalize(ϕ, name)\n",
    "                push!(ϕs_after_evidence, ϕ_after_marginalizing)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    normalize!(prod(ϕs_after_evidence))\n",
    "end\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?deleteat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?findall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test the new `infer` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = Variable(:b, 2)\n",
    "S = Variable(:s, 2)\n",
    "E = Variable(:e, 2)\n",
    "D = Variable(:d, 2)\n",
    "C = Variable(:c, 2)\n",
    "\n",
    "vars = [B,S,E,D,C]\n",
    "\n",
    "factorB = Factor([B], FactorTable((b=1,)=>0.99, (b=2,)=>0.01))\n",
    "factorS = Factor([S], FactorTable((s=1,)=>0.98, (s=2,)=>0.02))\n",
    "factorE = Factor([B,S,E], FactorTable(\n",
    "        (e=1,b=1,s=1) => 0.90,\n",
    "        (e=1,b=1,s=2) => 0.04,\n",
    "        (e=1,b=2,s=1) => 0.05,\n",
    "        (e=1,b=2,s=2) => 0.01,\n",
    "        (e=2,b=1,s=1) => 0.10,\n",
    "        (e=2,b=1,s=2) => 0.96,\n",
    "        (e=2,b=2,s=1) => 0.95,\n",
    "        (e=2,b=2,s=2) => 0.99,\n",
    "        ))\n",
    "factorD = Factor([D,E], FactorTable(\n",
    "        (d=1,e=1) => 0.96,\n",
    "        (d=1,e=2) => 0.03,\n",
    "        (d=2,e=1) => 0.04,\n",
    "        (d=2,e=2) => 0.97,\n",
    "        ))\n",
    "factorC = Factor([C,E], FactorTable(\n",
    "        (c=1,e=1) => 0.98,\n",
    "        (c=1,e=2) => 0.01,\n",
    "        (c=2,e=1) => 0.02,\n",
    "        (c=2,e=2) => 0.99,\n",
    "        ))\n",
    "\n",
    "graph = SimpleDiGraph(5)\n",
    "\n",
    "add_edge!(graph, 1, 3)\n",
    "add_edge!(graph, 2, 3)\n",
    "add_edge!(graph, 3, 4)\n",
    "add_edge!(graph, 3, 5)\n",
    "\n",
    "bn = BayesianNetwork(vars, [factorB, factorS, factorE, factorD, factorC], graph);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable[Variable(:b, 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Dict{Symbol, Int64}, Float64} with 2 entries:\n",
       "  Dict(:b=>2) => 0.0753055\n",
       "  Dict(:b=>1) => 0.924695"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = VariableElimination([3,2]) # eliminate E then S\n",
    "evidence = (d=2,c=2)\n",
    "factor = infer(M, bn, [B.name], evidence)\n",
    "println(factor.vars)\n",
    "factor.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct sampling\n",
    "\n",
    "Idea: Random samples from the joint distribution are used to arrive at a probability estimate.\n",
    "\n",
    "Suppose we want to infer $P(b^1 | d^1, c^1)$ from a set of `n` samples from the joint distribution `P(b,s,e,d,c)`. The direct sample estimate is:\n",
    "\n",
    "$$ P(b^1 | d^1, c^1) ≈ \\frac{\\sum_i (b^{(i)} = 1 ∧ d^{(i)} = 1 ∧ c^{(i)} = 1)}{\\sum_i (d^{(i)} = 1 ∧ c^{(i)} = 1)} $$\n",
    "\n",
    "The first step involves finding a topological sort of the nodes in the Bayesian network. A topological sort always exists, but it may not be unique.\n",
    "\n",
    "Once we have a topological sort, we can begin sampling from the conditional probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.rand(ϕ::Factor)\n",
    "    # the idea is to return a random assignment from ϕ\n",
    "    # the assignment is chosen based on the `random_number`\n",
    "    # note that this will result in assignments with high probability being returned more often\n",
    "    random_number = rand()\n",
    "    total_prob = sum(values(ϕ.table))\n",
    "    total = 0.0\n",
    "    for (assignment, prob) in ϕ.table\n",
    "        total += prob / total_prob\n",
    "        if total >= random_number\n",
    "            return assignment\n",
    "        end\n",
    "    end\n",
    "    return Assignment()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5, 4} directed simple Int64 graph"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Int64}:\n",
       " 2\n",
       " 1\n",
       " 3\n",
       " 5\n",
       " 4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topological_sort_by_dfs(bn.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the below function in greater detail. Some points to note:\n",
    "\n",
    "- `condition` function does not normalize the factors\n",
    "- `rand` is applied to conditioned output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.rand(bn::BayesianNetwork)\n",
    "    output = Assignment()\n",
    "    for i in topological_sort_by_dfs(bn.graph)\n",
    "        variable, ϕ = bn.vars[i].name, bn.factors[i]\n",
    "        random_assignment_from_factor = rand(condition(ϕ, output))\n",
    "        output[variable] = random_assignment_from_factor[variable]\n",
    "    end\n",
    "    return output\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "infer (generic function with 3 methods)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct DirectSampling\n",
    "    m #number of samples\n",
    "end\n",
    "\n",
    "function infer(M::DirectSampling, bn, query, evidence)\n",
    "    table = FactorTable()\n",
    "    for i in 1:(M.m)\n",
    "        assignment = rand(bn)\n",
    "        if all(assignment[k] == v for (k,v) in pairs(evidence))\n",
    "            sub_assignment = select(assignment, query)\n",
    "            table[sub_assignment] = get(table, sub_assignment, 0) + 1\n",
    "        end\n",
    "    end\n",
    "    vars = filter(v -> v.name ∈ query, bn.vars)\n",
    "    return normalize!(Factor(vars, table))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the empirical estimation with the actual values:\n",
    "\n",
    "```julia\n",
    "  Dict(:b=>2) => 0.0753055\n",
    "  Dict(:b=>1) => 0.924695\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 1) => 1.0))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(DirectSampling(100), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 2) => 0.05785123966942149, Dict(:b => 1) => 0.9421487603305785))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(DirectSampling(1000), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 2) => 0.0785296574770259, Dict(:b => 1) => 0.9214703425229741))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(DirectSampling(10000), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 2) => 0.07581816697951523, Dict(:b => 1) => 0.9241818330204847))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(DirectSampling(100000), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood Weighted Sampling\n",
    "\n",
    "Each sample `i` is given a weight $w_i$. This gives us:\n",
    "\n",
    "$$ P(b^1 | d^1, c^1) ≈ \\frac{\\sum_i w_i(b^{(i)} = 1 ∧ d^{(i)} = 1 ∧ c^{(i)} = 1)}{\\sum_i w_i(d^{(i)} = 1 ∧ c^{(i)} = 1)} $$\n",
    "\n",
    "In this case we don't really sample all the variables. The evidence variables are set to their specified values. Eg in the graph above, `D` and `C` are not sampled. They are set equal to the evidence `d=1` and `c=1`. But `d=1` happens only `P(d=1|e)` fraction of the time. Similarly `c=1` happens only `P(c=1|e)` fraction of the time. Thus we multiply each sample with the weight `P(d=1|e) * P(d=c|e)` for the given value of `e` in the sample.\n",
    "\n",
    "$$ = \\frac{\\sum_i w_i(b^{(i)} = 1)}{\\sum_i w_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct LikelihoodWeightedSampling\n",
    "    m # number of samples\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "infer (generic function with 4 methods)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function infer(M::LikelihoodWeightedSampling, bn, query, evidence)\n",
    "    table = FactorTable()\n",
    "    ordering = topological_sort_by_dfs(bn.graph)\n",
    "    for i in 1:M.m\n",
    "        assignment, weight = Assignment(), 1.0\n",
    "        for j in ordering\n",
    "            variable_name, ϕ = bn.vars[j].name, bn.factors[j]\n",
    "            if haskey(evidence, variable_name)\n",
    "                assignment[variable_name] = evidence[variable_name]\n",
    "                weight *= ϕ.table[select(assignment, variablenames(ϕ))]\n",
    "            else\n",
    "                assignment[variable_name] = rand(condition(ϕ,assignment))[variable_name]\n",
    "            end\n",
    "        end\n",
    "        sub_assignment = select(assignment, query)\n",
    "        table[sub_assignment] = get(table, sub_assignment, 0) + weight\n",
    "    end\n",
    "    vars = filter(v -> v.name ∈ query, bn.vars)\n",
    "    return normalize!(Factor(vars, table))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 1) => 1.0))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(LikelihoodWeightedSampling(10), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 1) => 1.0))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(LikelihoodWeightedSampling(100), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 2) => 0.0374033882730401, Dict(:b => 1) => 0.9625966117269599))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(LikelihoodWeightedSampling(1000), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 2) => 0.09088823398927763, Dict(:b => 1) => 0.9091117660107224))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(LikelihoodWeightedSampling(10000), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 2) => 0.07540498135116384, Dict(:b => 1) => 0.9245950186488362))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(LikelihoodWeightedSampling(100000), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although likelihood weighting makes it so that all samples are consistent with the observations, it can still be wasteful. Consider the simple chemical detection Bayesian network shown in figure below:\n",
    "\n",
    "![](assets/fig_3.5.png)\n",
    "\n",
    "One can easily see that $P(c^1 | d^1) = 0.5$. \n",
    "\n",
    "If we use likelihood weighting, then 99.9% of the samples will have `C = 0` with a weight of 0.001. Until we get a sample of `C = 1`, which has an associated weight of 0.999, our estimate of $P(c^1 | d^1)$ will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampling\n",
    "\n",
    "Gibbs sampling involves drawing samples consistent with the evidence in a way that does not involve weighting. From these samples, we can infer the distribution over the query variables.\n",
    "\n",
    "Using any ordering of the unobserved variables, assign to $X_i^{(k)}$ a value from the distribution presented by $P(X_i | x_{-i}^{(k)})$. Here $ x_{-i}^{(k)}$ represents the values of all other variables except $X_i$ in the `k-th` sample.\n",
    "\n",
    "> Sampling from $P(X_i | x_{-i}^{(k)})$ can be done efficiently because we only need to consider the Markov blanket of $X_i$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "function blanket(bn, assignment, index)\n",
    "    # `assignment` is an assignment over all variables\n",
    "    # ... in the bayesian network `bn`\n",
    "    name = bn.vars[index].name\n",
    "    assignment_without_variable = delete!(copy(assignment), name)\n",
    "    factors_containing_name = filter(ϕ -> in_scope(name, ϕ), bn.factors)\n",
    "    ϕ = prod(condition(ϕ, assignment_without_variable) for ϕ in factors_containing_name)\n",
    "    return normalize!(ϕ)\n",
    "end\n",
    "\n",
    "function update_gibbs_sample!(assignment, bn, evidence, ordering)\n",
    "    for index in ordering\n",
    "        name = bn.vars[index].name\n",
    "        if !haskey(evidence, name)\n",
    "            assignment_of_name = blanket(bn, assignment, index)\n",
    "            assignment[name] = rand(assignment_of_name)[name]\n",
    "        end\n",
    "    end\n",
    "    return assignment\n",
    "end\n",
    "    \n",
    "function gibbs_sample(assignment, bn, evidence, ordering, m)\n",
    "    for j in 1:m\n",
    "        assignment = update_gibbs_sample!(assignment, bn, evidence, ordering)\n",
    "    end\n",
    "    return assignment\n",
    "end\n",
    "\n",
    "struct GibbsSampling\n",
    "    m_samples\n",
    "    m_burn_in\n",
    "    m_skip\n",
    "    ordering\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "infer (generic function with 5 methods)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function infer(M::GibbsSampling, bn, query, evidence)\n",
    "    table = FactorTable()\n",
    "    assignment = merge(rand(bn), Assignment(evidence))\n",
    "    assignment = gibbs_sample(assignment, bn, evidence, M.ordering, M.m_burn_in)\n",
    "    for i in 1:(M.m_samples)\n",
    "        assignment = gibbs_sample(assignment, bn, evidence, M.ordering, M.m_burn_in)\n",
    "        sub_assignment = select(assignment, query)\n",
    "        table[sub_assignment] = get(table, sub_assignment, 0.0) + 1\n",
    "    end\n",
    "    vars = filter(v -> v.name ∈ query, bn.vars)\n",
    "    return normalize!(Factor(vars, table))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 2) => 0.05, Dict(:b => 1) => 0.95))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(GibbsSampling(100, 1, 1, 1:length(bn.vars)), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 2) => 0.08, Dict(:b => 1) => 0.92))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(GibbsSampling(100, 10, 1, 1:length(bn.vars)), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 2) => 0.07, Dict(:b => 1) => 0.93))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(GibbsSampling(100, 100, 1, 1:length(bn.vars)), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 2) => 0.078, Dict(:b => 1) => 0.922))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(GibbsSampling(1000, 10, 1, 1:length(bn.vars)), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:b, 2)], Dict(Dict(:b => 2) => 0.0724, Dict(:b => 1) => 0.9276))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(GibbsSampling(10000, 10, 1, 1:length(bn.vars)), bn, [B.name], (d=2, c=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test it on the following Bayesian network\n",
    "\n",
    "![](assets/fig_3.5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2 = Variable(:c2, 2)\n",
    "D2 = Variable(:d2, 2)\n",
    "\n",
    "factorC2 = Factor([C2], FactorTable((c2=1,)=>0.999, (c2=2,)=>0.001))\n",
    "factorD2 = Factor([C2,D2], FactorTable(\n",
    "        (c2=1,d2=1) => 0.999,\n",
    "        (c2=1,d2=2) => 0.001,\n",
    "        (c2=2,d2=1) => 0.001,\n",
    "        (c2=2,d2=2) => 0.999,\n",
    "        ))\n",
    "\n",
    "graph = SimpleDiGraph(2)\n",
    "\n",
    "add_edge!(graph, 1, 2)\n",
    "\n",
    "bn = BayesianNetwork([C2,D2], [factorC2, factorD2], graph);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:c2, 2)], Dict(Dict(:c2 => 1) => 0.5, Dict(:c2 => 2) => 0.5))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(ExactInference(), bn, [C2.name], (d2=2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:c2, 2)], Dict{Dict{Symbol, Int64}, Float64}())"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(DirectSampling(100), bn, [C2.name], (d2=2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:c2, 2)], Dict(Dict(:c2 => 1) => 1.0))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(LikelihoodWeightedSampling(100), bn, [C2.name], (d2=2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:c2, 2)], Dict(Dict(:c2 => 1) => 0.5, Dict(:c2 => 2) => 0.5))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(GibbsSampling(50, 10, 1, [1,2]), bn, [C2.name], (d2=2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:c2, 2)], Dict(Dict(:c2 => 1) => 0.514, Dict(:c2 => 2) => 0.486))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(GibbsSampling(500, 10, 1, [1,2]), bn, [C2.name], (d2=2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor(Variable[Variable(:c2, 2)], Dict(Dict(:c2 => 1) => 0.522, Dict(:c2 => 2) => 0.478))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(GibbsSampling(1000, 10, 1, [1,2]), bn, [C2.name], (d2=2,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the comparison of `DirectSampling`, `Likelihood Weighted Sampling` and `Gibbs Sampling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = [get(infer(DirectSampling(i), bn, [C2.name], (d2=2,)).table, Dict(:c2 => 2), 0) for i in 1:100:10000]\n",
    "temp2 = [get(infer(LikelihoodWeightedSampling(i), bn, [C2.name], (d2=2,)).table, Dict(:c2 => 2), 0) for i in 1:100:10000]\n",
    "temp3 = [get(infer(GibbsSampling(i, 10, 1, [1,2]), bn, [C2.name], (d2=2,)).table, Dict(:c2 => 2), 0) for i in 1:100:10000];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip230\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip230)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip231\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip230)\" d=\"\n",
       "M186.274 1486.45 L2352.76 1486.45 L2352.76 47.2441 L186.274 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip232\">\n",
       "    <rect x=\"186\" y=\"47\" width=\"2167\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip232)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.945,1486.45 226.945,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip232)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  743.069,1486.45 743.069,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip232)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1259.19,1486.45 1259.19,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip232)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1775.32,1486.45 1775.32,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip232)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2291.44,1486.45 2291.44,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.945,1486.45 226.945,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  743.069,1486.45 743.069,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1259.19,1486.45 1259.19,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1775.32,1486.45 1775.32,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2291.44,1486.45 2291.44,1469.18 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip230)\" d=\"M 0 0 M226.945 1515.64 Q223.333 1515.64 221.505 1519.2 Q219.699 1522.75 219.699 1529.87 Q219.699 1536.98 221.505 1540.55 Q223.333 1544.09 226.945 1544.09 Q230.579 1544.09 232.384 1540.55 Q234.213 1536.98 234.213 1529.87 Q234.213 1522.75 232.384 1519.2 Q230.579 1515.64 226.945 1515.64 M226.945 1511.93 Q232.755 1511.93 235.81 1516.54 Q238.889 1521.12 238.889 1529.87 Q238.889 1538.6 235.81 1543.21 Q232.755 1547.79 226.945 1547.79 Q221.134 1547.79 218.056 1543.21 Q215 1538.6 215 1529.87 Q215 1521.12 218.056 1516.54 Q221.134 1511.93 226.945 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M722.339 1543.18 L738.659 1543.18 L738.659 1547.12 L716.714 1547.12 L716.714 1543.18 Q719.377 1540.43 723.96 1535.8 Q728.566 1531.15 729.747 1529.81 Q731.992 1527.28 732.872 1525.55 Q733.775 1523.79 733.775 1522.1 Q733.775 1519.34 731.83 1517.61 Q729.909 1515.87 726.807 1515.87 Q724.608 1515.87 722.154 1516.63 Q719.724 1517.4 716.946 1518.95 L716.946 1514.23 Q719.77 1513.09 722.224 1512.51 Q724.677 1511.93 726.714 1511.93 Q732.085 1511.93 735.279 1514.62 Q738.474 1517.31 738.474 1521.8 Q738.474 1523.93 737.663 1525.85 Q736.876 1527.74 734.77 1530.34 Q734.191 1531.01 731.089 1534.23 Q727.988 1537.42 722.339 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M748.52 1512.56 L766.876 1512.56 L766.876 1516.5 L752.802 1516.5 L752.802 1524.97 Q753.821 1524.62 754.839 1524.46 Q755.858 1524.27 756.876 1524.27 Q762.663 1524.27 766.043 1527.44 Q769.423 1530.62 769.423 1536.03 Q769.423 1541.61 765.95 1544.71 Q762.478 1547.79 756.159 1547.79 Q753.983 1547.79 751.714 1547.42 Q749.469 1547.05 747.062 1546.31 L747.062 1541.61 Q749.145 1542.74 751.367 1543.3 Q753.589 1543.86 756.066 1543.86 Q760.071 1543.86 762.409 1541.75 Q764.747 1539.64 764.747 1536.03 Q764.747 1532.42 762.409 1530.31 Q760.071 1528.21 756.066 1528.21 Q754.191 1528.21 752.316 1528.62 Q750.464 1529.04 748.52 1529.92 L748.52 1512.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M1233.89 1512.56 L1252.25 1512.56 L1252.25 1516.5 L1238.17 1516.5 L1238.17 1524.97 Q1239.19 1524.62 1240.21 1524.46 Q1241.23 1524.27 1242.25 1524.27 Q1248.04 1524.27 1251.41 1527.44 Q1254.79 1530.62 1254.79 1536.03 Q1254.79 1541.61 1251.32 1544.71 Q1247.85 1547.79 1241.53 1547.79 Q1239.35 1547.79 1237.09 1547.42 Q1234.84 1547.05 1232.43 1546.31 L1232.43 1541.61 Q1234.52 1542.74 1236.74 1543.3 Q1238.96 1543.86 1241.44 1543.86 Q1245.44 1543.86 1247.78 1541.75 Q1250.12 1539.64 1250.12 1536.03 Q1250.12 1532.42 1247.78 1530.31 Q1245.44 1528.21 1241.44 1528.21 Q1239.56 1528.21 1237.69 1528.62 Q1235.84 1529.04 1233.89 1529.92 L1233.89 1512.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M1274.01 1515.64 Q1270.4 1515.64 1268.57 1519.2 Q1266.76 1522.75 1266.76 1529.87 Q1266.76 1536.98 1268.57 1540.55 Q1270.4 1544.09 1274.01 1544.09 Q1277.64 1544.09 1279.45 1540.55 Q1281.28 1536.98 1281.28 1529.87 Q1281.28 1522.75 1279.45 1519.2 Q1277.64 1515.64 1274.01 1515.64 M1274.01 1511.93 Q1279.82 1511.93 1282.87 1516.54 Q1285.95 1521.12 1285.95 1529.87 Q1285.95 1538.6 1282.87 1543.21 Q1279.82 1547.79 1274.01 1547.79 Q1268.2 1547.79 1265.12 1543.21 Q1262.06 1538.6 1262.06 1529.87 Q1262.06 1521.12 1265.12 1516.54 Q1268.2 1511.93 1274.01 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M1749.17 1512.56 L1771.39 1512.56 L1771.39 1514.55 L1758.85 1547.12 L1753.96 1547.12 L1765.77 1516.5 L1749.17 1516.5 L1749.17 1512.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M1780.56 1512.56 L1798.92 1512.56 L1798.92 1516.5 L1784.84 1516.5 L1784.84 1524.97 Q1785.86 1524.62 1786.88 1524.46 Q1787.9 1524.27 1788.92 1524.27 Q1794.7 1524.27 1798.08 1527.44 Q1801.46 1530.62 1801.46 1536.03 Q1801.46 1541.61 1797.99 1544.71 Q1794.52 1547.79 1788.2 1547.79 Q1786.02 1547.79 1783.75 1547.42 Q1781.51 1547.05 1779.1 1546.31 L1779.1 1541.61 Q1781.18 1542.74 1783.41 1543.3 Q1785.63 1543.86 1788.11 1543.86 Q1792.11 1543.86 1794.45 1541.75 Q1796.79 1539.64 1796.79 1536.03 Q1796.79 1532.42 1794.45 1530.31 Q1792.11 1528.21 1788.11 1528.21 Q1786.23 1528.21 1784.36 1528.62 Q1782.5 1529.04 1780.56 1529.92 L1780.56 1512.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M2251.05 1543.18 L2258.69 1543.18 L2258.69 1516.82 L2250.38 1518.49 L2250.38 1514.23 L2258.64 1512.56 L2263.32 1512.56 L2263.32 1543.18 L2270.95 1543.18 L2270.95 1547.12 L2251.05 1547.12 L2251.05 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M2290.4 1515.64 Q2286.79 1515.64 2284.96 1519.2 Q2283.15 1522.75 2283.15 1529.87 Q2283.15 1536.98 2284.96 1540.55 Q2286.79 1544.09 2290.4 1544.09 Q2294.03 1544.09 2295.84 1540.55 Q2297.67 1536.98 2297.67 1529.87 Q2297.67 1522.75 2295.84 1519.2 Q2294.03 1515.64 2290.4 1515.64 M2290.4 1511.93 Q2296.21 1511.93 2299.26 1516.54 Q2302.34 1521.12 2302.34 1529.87 Q2302.34 1538.6 2299.26 1543.21 Q2296.21 1547.79 2290.4 1547.79 Q2284.59 1547.79 2281.51 1543.21 Q2278.45 1538.6 2278.45 1529.87 Q2278.45 1521.12 2281.51 1516.54 Q2284.59 1511.93 2290.4 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M2320.56 1515.64 Q2316.95 1515.64 2315.12 1519.2 Q2313.32 1522.75 2313.32 1529.87 Q2313.32 1536.98 2315.12 1540.55 Q2316.95 1544.09 2320.56 1544.09 Q2324.19 1544.09 2326 1540.55 Q2327.83 1536.98 2327.83 1529.87 Q2327.83 1522.75 2326 1519.2 Q2324.19 1515.64 2320.56 1515.64 M2320.56 1511.93 Q2326.37 1511.93 2329.43 1516.54 Q2332.5 1521.12 2332.5 1529.87 Q2332.5 1538.6 2329.43 1543.21 Q2326.37 1547.79 2320.56 1547.79 Q2314.75 1547.79 2311.67 1543.21 Q2308.62 1538.6 2308.62 1529.87 Q2308.62 1521.12 2311.67 1516.54 Q2314.75 1511.93 2320.56 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip232)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  186.274,1445.72 2352.76,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip232)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  186.274,1106.28 2352.76,1106.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip232)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  186.274,766.846 2352.76,766.846 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip232)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  186.274,427.411 2352.76,427.411 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip232)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  186.274,87.9763 2352.76,87.9763 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,1486.45 186.274,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,1445.72 212.272,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,1106.28 212.272,1106.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,766.846 212.272,766.846 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,427.411 212.272,427.411 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,87.9763 212.272,87.9763 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip230)\" d=\"M 0 0 M62.9365 1431.51 Q59.3254 1431.51 57.4967 1435.08 Q55.6912 1438.62 55.6912 1445.75 Q55.6912 1452.86 57.4967 1456.42 Q59.3254 1459.96 62.9365 1459.96 Q66.5707 1459.96 68.3763 1456.42 Q70.205 1452.86 70.205 1445.75 Q70.205 1438.62 68.3763 1435.08 Q66.5707 1431.51 62.9365 1431.51 M62.9365 1427.81 Q68.7467 1427.81 71.8022 1432.42 Q74.8809 1437 74.8809 1445.75 Q74.8809 1454.48 71.8022 1459.08 Q68.7467 1463.67 62.9365 1463.67 Q57.1264 1463.67 54.0477 1459.08 Q50.9921 1454.48 50.9921 1445.75 Q50.9921 1437 54.0477 1432.42 Q57.1264 1427.81 62.9365 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M83.0984 1457.12 L87.9827 1457.12 L87.9827 1463 L83.0984 1463 L83.0984 1457.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M108.168 1431.51 Q104.557 1431.51 102.728 1435.08 Q100.922 1438.62 100.922 1445.75 Q100.922 1452.86 102.728 1456.42 Q104.557 1459.96 108.168 1459.96 Q111.802 1459.96 113.608 1456.42 Q115.436 1452.86 115.436 1445.75 Q115.436 1438.62 113.608 1435.08 Q111.802 1431.51 108.168 1431.51 M108.168 1427.81 Q113.978 1427.81 117.033 1432.42 Q120.112 1437 120.112 1445.75 Q120.112 1454.48 117.033 1459.08 Q113.978 1463.67 108.168 1463.67 Q102.358 1463.67 99.2789 1459.08 Q96.2234 1454.48 96.2234 1445.75 Q96.2234 1437 99.2789 1432.42 Q102.358 1427.81 108.168 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M138.33 1431.51 Q134.719 1431.51 132.89 1435.08 Q131.084 1438.62 131.084 1445.75 Q131.084 1452.86 132.89 1456.42 Q134.719 1459.96 138.33 1459.96 Q141.964 1459.96 143.769 1456.42 Q145.598 1452.86 145.598 1445.75 Q145.598 1438.62 143.769 1435.08 Q141.964 1431.51 138.33 1431.51 M138.33 1427.81 Q144.14 1427.81 147.195 1432.42 Q150.274 1437 150.274 1445.75 Q150.274 1454.48 147.195 1459.08 Q144.14 1463.67 138.33 1463.67 Q132.519 1463.67 129.441 1459.08 Q126.385 1454.48 126.385 1445.75 Q126.385 1437 129.441 1432.42 Q132.519 1427.81 138.33 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M63.9319 1092.08 Q60.3208 1092.08 58.4921 1095.64 Q56.6865 1099.19 56.6865 1106.32 Q56.6865 1113.42 58.4921 1116.99 Q60.3208 1120.53 63.9319 1120.53 Q67.5661 1120.53 69.3717 1116.99 Q71.2004 1113.42 71.2004 1106.32 Q71.2004 1099.19 69.3717 1095.64 Q67.5661 1092.08 63.9319 1092.08 M63.9319 1088.38 Q69.742 1088.38 72.7976 1092.98 Q75.8763 1097.57 75.8763 1106.32 Q75.8763 1115.04 72.7976 1119.65 Q69.742 1124.23 63.9319 1124.23 Q58.1217 1124.23 55.043 1119.65 Q51.9875 1115.04 51.9875 1106.32 Q51.9875 1097.57 55.043 1092.98 Q58.1217 1088.38 63.9319 1088.38 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M84.0938 1117.68 L88.978 1117.68 L88.978 1123.56 L84.0938 1123.56 L84.0938 1117.68 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M103.191 1119.63 L119.51 1119.63 L119.51 1123.56 L97.566 1123.56 L97.566 1119.63 Q100.228 1116.87 104.811 1112.24 Q109.418 1107.59 110.598 1106.25 Q112.844 1103.72 113.723 1101.99 Q114.626 1100.23 114.626 1098.54 Q114.626 1095.78 112.682 1094.05 Q110.76 1092.31 107.658 1092.31 Q105.459 1092.31 103.006 1093.07 Q100.575 1093.84 97.7974 1095.39 L97.7974 1090.67 Q100.621 1089.53 103.075 1088.95 Q105.529 1088.38 107.566 1088.38 Q112.936 1088.38 116.131 1091.06 Q119.325 1093.75 119.325 1098.24 Q119.325 1100.37 118.515 1102.29 Q117.728 1104.19 115.621 1106.78 Q115.043 1107.45 111.941 1110.67 Q108.839 1113.86 103.191 1119.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M129.371 1089 L147.728 1089 L147.728 1092.94 L133.654 1092.94 L133.654 1101.41 Q134.672 1101.06 135.691 1100.9 Q136.709 1100.71 137.728 1100.71 Q143.515 1100.71 146.894 1103.89 Q150.274 1107.06 150.274 1112.47 Q150.274 1118.05 146.802 1121.15 Q143.33 1124.23 137.01 1124.23 Q134.834 1124.23 132.566 1123.86 Q130.32 1123.49 127.913 1122.75 L127.913 1118.05 Q129.996 1119.19 132.219 1119.74 Q134.441 1120.3 136.918 1120.3 Q140.922 1120.3 143.26 1118.19 Q145.598 1116.08 145.598 1112.47 Q145.598 1108.86 143.26 1106.76 Q140.922 1104.65 136.918 1104.65 Q135.043 1104.65 133.168 1105.07 Q131.316 1105.48 129.371 1106.36 L129.371 1089 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M62.9365 752.645 Q59.3254 752.645 57.4967 756.209 Q55.6912 759.751 55.6912 766.881 Q55.6912 773.987 57.4967 777.552 Q59.3254 781.094 62.9365 781.094 Q66.5707 781.094 68.3763 777.552 Q70.205 773.987 70.205 766.881 Q70.205 759.751 68.3763 756.209 Q66.5707 752.645 62.9365 752.645 M62.9365 748.941 Q68.7467 748.941 71.8022 753.547 Q74.8809 758.131 74.8809 766.881 Q74.8809 775.608 71.8022 780.214 Q68.7467 784.797 62.9365 784.797 Q57.1264 784.797 54.0477 780.214 Q50.9921 775.608 50.9921 766.881 Q50.9921 758.131 54.0477 753.547 Q57.1264 748.941 62.9365 748.941 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M83.0984 778.246 L87.9827 778.246 L87.9827 784.126 L83.0984 784.126 L83.0984 778.246 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M98.2141 749.566 L116.57 749.566 L116.57 753.501 L102.496 753.501 L102.496 761.973 Q103.515 761.626 104.534 761.464 Q105.552 761.279 106.571 761.279 Q112.358 761.279 115.737 764.45 Q119.117 767.621 119.117 773.038 Q119.117 778.617 115.645 781.719 Q112.172 784.797 105.853 784.797 Q103.677 784.797 101.409 784.427 Q99.1632 784.057 96.7558 783.316 L96.7558 778.617 Q98.8391 779.751 101.061 780.307 Q103.284 780.862 105.76 780.862 Q109.765 780.862 112.103 778.756 Q114.441 776.649 114.441 773.038 Q114.441 769.427 112.103 767.321 Q109.765 765.214 105.76 765.214 Q103.885 765.214 102.01 765.631 Q100.159 766.047 98.2141 766.927 L98.2141 749.566 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M138.33 752.645 Q134.719 752.645 132.89 756.209 Q131.084 759.751 131.084 766.881 Q131.084 773.987 132.89 777.552 Q134.719 781.094 138.33 781.094 Q141.964 781.094 143.769 777.552 Q145.598 773.987 145.598 766.881 Q145.598 759.751 143.769 756.209 Q141.964 752.645 138.33 752.645 M138.33 748.941 Q144.14 748.941 147.195 753.547 Q150.274 758.131 150.274 766.881 Q150.274 775.608 147.195 780.214 Q144.14 784.797 138.33 784.797 Q132.519 784.797 129.441 780.214 Q126.385 775.608 126.385 766.881 Q126.385 758.131 129.441 753.547 Q132.519 748.941 138.33 748.941 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M63.9319 413.21 Q60.3208 413.21 58.4921 416.775 Q56.6865 420.316 56.6865 427.446 Q56.6865 434.552 58.4921 438.117 Q60.3208 441.659 63.9319 441.659 Q67.5661 441.659 69.3717 438.117 Q71.2004 434.552 71.2004 427.446 Q71.2004 420.316 69.3717 416.775 Q67.5661 413.21 63.9319 413.21 M63.9319 409.506 Q69.742 409.506 72.7976 414.113 Q75.8763 418.696 75.8763 427.446 Q75.8763 436.173 72.7976 440.779 Q69.742 445.362 63.9319 445.362 Q58.1217 445.362 55.043 440.779 Q51.9875 436.173 51.9875 427.446 Q51.9875 418.696 55.043 414.113 Q58.1217 409.506 63.9319 409.506 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M84.0938 438.812 L88.978 438.812 L88.978 444.691 L84.0938 444.691 L84.0938 438.812 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M97.9826 410.131 L120.205 410.131 L120.205 412.122 L107.658 444.691 L102.774 444.691 L114.58 414.066 L97.9826 414.066 L97.9826 410.131 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M129.371 410.131 L147.728 410.131 L147.728 414.066 L133.654 414.066 L133.654 422.538 Q134.672 422.191 135.691 422.029 Q136.709 421.844 137.728 421.844 Q143.515 421.844 146.894 425.015 Q150.274 428.187 150.274 433.603 Q150.274 439.182 146.802 442.284 Q143.33 445.362 137.01 445.362 Q134.834 445.362 132.566 444.992 Q130.32 444.622 127.913 443.881 L127.913 439.182 Q129.996 440.316 132.219 440.872 Q134.441 441.427 136.918 441.427 Q140.922 441.427 143.26 439.321 Q145.598 437.214 145.598 433.603 Q145.598 429.992 143.26 427.886 Q140.922 425.779 136.918 425.779 Q135.043 425.779 133.168 426.196 Q131.316 426.613 129.371 427.492 L129.371 410.131 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M53.7467 101.321 L61.3856 101.321 L61.3856 74.9555 L53.0754 76.6222 L53.0754 72.3629 L61.3393 70.6963 L66.0152 70.6963 L66.0152 101.321 L73.654 101.321 L73.654 105.256 L53.7467 105.256 L53.7467 101.321 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M83.0984 99.3767 L87.9827 99.3767 L87.9827 105.256 L83.0984 105.256 L83.0984 99.3767 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M108.168 73.775 Q104.557 73.775 102.728 77.3398 Q100.922 80.8814 100.922 88.011 Q100.922 95.1174 102.728 98.6822 Q104.557 102.224 108.168 102.224 Q111.802 102.224 113.608 98.6822 Q115.436 95.1174 115.436 88.011 Q115.436 80.8814 113.608 77.3398 Q111.802 73.775 108.168 73.775 M108.168 70.0713 Q113.978 70.0713 117.033 74.6777 Q120.112 79.261 120.112 88.011 Q120.112 96.7378 117.033 101.344 Q113.978 105.928 108.168 105.928 Q102.358 105.928 99.2789 101.344 Q96.2234 96.7378 96.2234 88.011 Q96.2234 79.261 99.2789 74.6777 Q102.358 70.0713 108.168 70.0713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M138.33 73.775 Q134.719 73.775 132.89 77.3398 Q131.084 80.8814 131.084 88.011 Q131.084 95.1174 132.89 98.6822 Q134.719 102.224 138.33 102.224 Q141.964 102.224 143.769 98.6822 Q145.598 95.1174 145.598 88.011 Q145.598 80.8814 143.769 77.3398 Q141.964 73.775 138.33 73.775 M138.33 70.0713 Q144.14 70.0713 147.195 74.6777 Q150.274 79.261 150.274 88.011 Q150.274 96.7378 147.195 101.344 Q144.14 105.928 138.33 105.928 Q132.519 105.928 129.441 101.344 Q126.385 96.7378 126.385 88.011 Q126.385 79.261 129.441 74.6777 Q132.519 70.0713 138.33 70.0713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip232)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  247.59,1445.72 268.234,1445.72 288.879,1445.72 309.524,1445.72 330.169,1445.72 350.814,1445.72 371.459,1445.72 392.104,87.9763 412.749,1445.72 433.394,1445.72 \n",
       "  454.039,540.556 474.684,1106.28 495.329,1445.72 515.974,1445.72 536.619,540.556 557.264,1445.72 577.909,993.136 598.554,902.62 619.199,1445.72 639.844,993.136 \n",
       "  660.489,766.846 681.134,842.276 701.779,1219.43 722.424,766.846 743.069,766.846 763.713,902.62 784.358,427.411 805.003,993.136 825.648,1057.79 846.293,597.129 \n",
       "  866.938,863.827 887.583,475.902 908.228,669.865 928.873,993.136 949.518,631.072 970.163,314.266 990.808,766.846 1011.45,257.694 1032.1,902.62 1052.74,828.561 \n",
       "  1073.39,1174.17 1094.03,902.62 1114.68,281.939 1135.32,1174.17 1155.97,766.846 1176.61,631.072 1197.26,281.939 1217.9,631.072 1238.55,653.701 1259.19,879.991 \n",
       "  1279.84,819.067 1300.48,610.184 1321.13,631.072 1341.77,631.072 1362.42,631.072 1383.06,653.701 1403.71,572.883 1424.35,1038.39 1445,610.184 1465.64,450.04 \n",
       "  1486.29,567.178 1506.93,572.883 1527.58,427.411 1548.22,705.131 1568.87,540.556 1589.51,581.7 1610.16,923.508 1630.8,879.991 1651.45,766.846 1672.09,721.588 \n",
       "  1692.74,842.276 1713.38,863.827 1734.03,812.104 1754.67,653.701 1775.32,597.129 1795.96,540.556 1816.61,879.991 1837.25,766.846 1857.9,936.563 1878.54,698.959 \n",
       "  1899.19,647.045 1919.83,819.067 1940.48,588.196 1961.12,643.415 1981.77,902.62 2002.41,631.072 2023.06,647.045 2043.7,734.519 2064.35,731.116 2084.99,653.701 \n",
       "  2105.64,1057.79 2126.28,659.656 2146.93,819.067 2167.57,643.415 2188.22,863.827 2208.86,396.553 2229.51,796.362 2250.15,734.519 2270.8,487.311 2291.44,973.459 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip232)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  247.59,1445.72 268.234,1445.72 288.879,1445.72 309.524,401.542 330.169,1445.72 350.814,1445.72 371.459,597.447 392.104,647.375 412.749,691.751 433.394,509.311 \n",
       "  454.039,767.186 474.684,1445.72 495.329,597.182 515.974,420.678 536.619,880.321 557.264,902.946 577.909,923.83 598.554,943.166 619.199,597.093 639.844,461.547 \n",
       "  660.489,631.072 681.134,1445.72 701.779,1021.71 722.424,677.227 743.069,597.049 763.713,705.198 784.358,855.6 805.003,868.163 825.648,880.203 846.293,1445.72 \n",
       "  866.938,902.837 887.583,1114.81 908.228,788.872 928.873,799.307 949.518,809.415 970.163,1445.72 990.808,961.034 1011.45,1445.72 1032.1,614.345 1052.74,758.331 \n",
       "  1073.39,691.416 1094.03,1000.78 1114.68,555.19 1135.32,887.917 1155.97,723.545 1176.61,731.154 1197.26,814.327 1217.9,916.912 1238.55,828.688 1259.19,566.408 \n",
       "  1279.84,766.914 1300.48,773.641 1321.13,666.634 1341.77,955.162 1362.42,961.005 1383.06,737.361 1403.71,880.144 1424.35,885.979 1445,891.695 1465.64,761.193 \n",
       "  1486.29,714.625 1506.93,675.318 1527.58,913.436 1548.22,686.104 1568.87,1012.6 1589.51,1017.16 1610.16,799.255 1630.8,938.325 1651.45,711.793 1672.09,677.131 \n",
       "  1692.74,721.588 1713.38,823.95 1734.03,889.411 1754.67,660.818 1775.32,740.415 1795.96,669.798 1816.61,642.648 1837.25,713.987 1857.9,758.288 1878.54,762.615 \n",
       "  1899.19,923.669 1919.83,563.841 1940.48,735.279 1961.12,739.39 1981.77,620.782 2002.41,1091.72 2023.06,1094.78 2043.7,950.36 2064.35,844.292 2084.99,727.331 \n",
       "  2105.64,766.884 2126.28,810.594 2146.93,967.814 2167.57,742.235 2188.22,1315.28 2208.86,825.124 2229.51,753.013 2250.15,876.73 2270.8,671.491 2291.44,883.468 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip232)\" style=\"stroke:#3da44d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  247.59,87.9763 268.234,760.125 288.879,736.449 309.524,814.209 330.169,738.066 350.814,806.142 371.459,788.308 392.104,769.751 412.749,747.353 433.394,763.079 \n",
       "  454.039,781.088 474.684,745.265 495.329,751.584 515.974,760.063 536.619,781.867 557.264,748.303 577.909,752.853 598.554,744.895 619.199,755.161 639.844,753.633 \n",
       "  660.489,788.22 681.134,750.367 701.779,752.966 722.424,745.309 743.069,767.694 763.713,769.289 784.358,762.931 805.003,764.584 825.648,768.058 846.293,771.292 \n",
       "  866.938,762.548 887.583,764.438 908.228,792.084 928.873,766.64 949.518,755.468 970.163,767.816 990.808,770.051 1011.45,768.13 1032.1,772.383 1052.74,772.241 \n",
       "  1073.39,762.265 1094.03,746.816 1114.68,773.795 1135.32,760.69 1155.97,779.341 1176.61,772.427 1197.26,764.043 1217.9,770.745 1238.55,760.766 1259.19,792.472 \n",
       "  1279.84,768.339 1300.48,777.36 1321.13,765.671 1341.77,762.364 1362.42,769.486 1383.06,764.748 1403.71,777.876 1424.35,767.68 1445,762.984 1465.64,766.731 \n",
       "  1486.29,776.914 1506.93,751.824 1527.58,784.691 1548.22,765.445 1568.87,774.164 1589.51,764.026 1610.16,765.92 1630.8,778.294 1651.45,767.145 1672.09,760.452 \n",
       "  1692.74,775.282 1713.38,781.664 1734.03,747.708 1754.67,765.823 1775.32,764.186 1795.96,771.281 1816.61,769.436 1837.25,769.931 1857.9,770.414 1878.54,783.085 \n",
       "  1899.19,772.531 1919.83,761.399 1940.48,756.002 1961.12,762.021 1981.77,753.674 2002.41,751.114 2023.06,774.502 2043.7,773.478 2064.35,763.375 2084.99,773.329 \n",
       "  2105.64,771.899 2126.28,756.776 2146.93,759.542 2167.57,754.803 2188.22,763.019 2208.86,774.063 2229.51,766.21 2250.15,751.521 2270.8,763.036 2291.44,767.737 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip230)\" d=\"\n",
       "M1986.81 337.138 L2280.54 337.138 L2280.54 95.2176 L1986.81 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1986.81,337.138 2280.54,337.138 2280.54,95.2176 1986.81,95.2176 1986.81,337.138 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip230)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2010.88,155.698 2155.31,155.698 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip230)\" d=\"M 0 0 M2193.23 175.385 Q2191.42 180.015 2189.71 181.427 Q2188 182.839 2185.13 182.839 L2181.72 182.839 L2181.72 179.274 L2184.22 179.274 Q2185.98 179.274 2186.96 178.44 Q2187.93 177.607 2189.11 174.505 L2189.87 172.561 L2179.39 147.052 L2183.9 147.052 L2192 167.329 L2200.1 147.052 L2204.62 147.052 L2193.23 175.385 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M2211.91 169.042 L2219.55 169.042 L2219.55 142.677 L2211.24 144.343 L2211.24 140.084 L2219.5 138.418 L2224.18 138.418 L2224.18 169.042 L2231.82 169.042 L2231.82 172.978 L2211.91 172.978 L2211.91 169.042 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip230)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2010.88,216.178 2155.31,216.178 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip230)\" d=\"M 0 0 M2193.23 235.865 Q2191.42 240.495 2189.71 241.907 Q2188 243.319 2185.13 243.319 L2181.72 243.319 L2181.72 239.754 L2184.22 239.754 Q2185.98 239.754 2186.96 238.92 Q2187.93 238.087 2189.11 234.985 L2189.87 233.041 L2179.39 207.532 L2183.9 207.532 L2192 227.809 L2200.1 207.532 L2204.62 207.532 L2193.23 235.865 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M2215.13 229.522 L2231.45 229.522 L2231.45 233.458 L2209.5 233.458 L2209.5 229.522 Q2212.16 226.768 2216.75 222.138 Q2221.35 217.485 2222.53 216.143 Q2224.78 213.62 2225.66 211.884 Q2226.56 210.124 2226.56 208.435 Q2226.56 205.68 2224.62 203.944 Q2222.7 202.208 2219.59 202.208 Q2217.4 202.208 2214.94 202.972 Q2212.51 203.735 2209.73 205.286 L2209.73 200.564 Q2212.56 199.43 2215.01 198.851 Q2217.47 198.273 2219.5 198.273 Q2224.87 198.273 2228.07 200.958 Q2231.26 203.643 2231.26 208.134 Q2231.26 210.263 2230.45 212.185 Q2229.66 214.083 2227.56 216.675 Q2226.98 217.347 2223.88 220.564 Q2220.78 223.759 2215.13 229.522 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip230)\" style=\"stroke:#3da44d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2010.88,276.658 2155.31,276.658 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip230)\" d=\"M 0 0 M2193.23 296.345 Q2191.42 300.975 2189.71 302.387 Q2188 303.799 2185.13 303.799 L2181.72 303.799 L2181.72 300.234 L2184.22 300.234 Q2185.98 300.234 2186.96 299.4 Q2187.93 298.567 2189.11 295.465 L2189.87 293.521 L2179.39 268.012 L2183.9 268.012 L2192 288.289 L2200.1 268.012 L2204.62 268.012 L2193.23 296.345 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip230)\" d=\"M 0 0 M2225.27 275.303 Q2228.62 276.021 2230.5 278.289 Q2232.4 280.558 2232.4 283.891 Q2232.4 289.007 2228.88 291.808 Q2225.36 294.609 2218.88 294.609 Q2216.7 294.609 2214.39 294.169 Q2212.09 293.752 2209.64 292.896 L2209.64 288.382 Q2211.59 289.516 2213.9 290.095 Q2216.22 290.674 2218.74 290.674 Q2223.14 290.674 2225.43 288.938 Q2227.74 287.201 2227.74 283.891 Q2227.74 280.836 2225.59 279.123 Q2223.46 277.387 2219.64 277.387 L2215.61 277.387 L2215.61 273.544 L2219.83 273.544 Q2223.28 273.544 2225.1 272.178 Q2226.93 270.79 2226.93 268.197 Q2226.93 265.535 2225.03 264.123 Q2223.16 262.688 2219.64 262.688 Q2217.72 262.688 2215.52 263.104 Q2213.32 263.521 2210.68 264.401 L2210.68 260.234 Q2213.34 259.493 2215.66 259.123 Q2218 258.753 2220.06 258.753 Q2225.38 258.753 2228.48 261.183 Q2231.59 263.59 2231.59 267.711 Q2231.59 270.581 2229.94 272.572 Q2228.3 274.54 2225.27 275.303 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(temp1)\n",
    "plot!(temp2)\n",
    "plot!(temp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in Gaussian Models\n",
    "\n",
    "If the joint distribution is Gaussian, we can perform exact inference analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MvNormal"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MvNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "infer (generic function with 6 methods)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function infer(D::MvNormal, query, evidencevars, evidence)\n",
    "    # A vector of integers specifies the query variables in the `query` argument\n",
    "    # A vector of integers specifies the evidence variables in the `evidencevars` argument\n",
    "    # The values of the evidence variables are contained in the vector `evidence`\n",
    "    μ, Σ = D.μ, D.Σ.mat\n",
    "    b, μa, μb = evidence, μ[query], μ[evidencevars]\n",
    "    A = Σ[query, query]\n",
    "    B = Σ[evidencevars, evidencevars]\n",
    "    C = Σ[query, evidencevars]\n",
    "    μ = μ[query] + C * (B\\(b-μb))\n",
    "    Σ = A - C * (B \\ C')\n",
    "    return MvNormal(μ, Σ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.227159   0.764097   0.466987  0.346521  0.611759\n",
       " 0.438928   0.0530954  0.166553  0.350378  0.203684\n",
       " 0.585236   0.520071   0.184206  0.886542  0.0302573\n",
       " 0.0152078  0.989532   0.232248  0.510437  0.107842\n",
       " 0.641432   0.46879    0.353757  0.520356  0.640759"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " 0.227159  0.466987  0.611759\n",
       " 0.585236  0.184206  0.0302573\n",
       " 0.641432  0.353757  0.640759"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[[1,3,5], [1,3,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullNormal(\n",
       "dim: 2\n",
       "μ: [0.0, 1.0]\n",
       "Σ: [3.0 1.0; 1.0 2.0]\n",
       ")\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example (Example 3.7 in the book)\n",
    "D = MvNormal([0.0,1.0], [3.0 1.0;1.0 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullNormal(\n",
       "dim: 1\n",
       "μ: [0.5]\n",
       "Σ: [2.5]\n",
       ")\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(D, [1], [2], [2.0]) # The conditional distribution for x1 given x2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
